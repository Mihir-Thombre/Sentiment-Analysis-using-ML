# -*- coding: utf-8 -*-
"""Sentiment Analysis_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13B5mljrP7RVe0IVNPVhRaTXgvZ2p5hsH

#Importing all the necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
warnings.filterwarnings("ignore", category=DeprecationWarning)
 
# %matplotlib inline
 
 
#regular expression 
 
import string
#string functions ke liye
 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
 
## Machine Learning Libraries
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
 
 
import warnings
warnings.filterwarnings("ignore")
 
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
 
#from google.colab import files
#files.upload()

#from google.colab import files
#files.upload()

test_data = pd.read_csv('test.csv')
train_data = pd.read_csv('train.csv')
train_original = train_data.copy()
test_original=test_data.copy()
test_data.shape
train_data.shape

ds = train_data.append(test_data,ignore_index=True,sort=True)

ds.head()

"""# Pre Processing the data"""

## Remove tweeter id

def remove_pattern(text,pattern):
    
    # re.findall() finds the pattern i.e @user and puts it in a list for further task
    r = re.findall(pattern,text)
    
    # re.sub() removes @user from the sentences in the dataset
    for i in r:
        text = re.sub(i,"",text)
    
    return text

ds['Tidy_Tweets'] = np.vectorize(remove_pattern)(ds['tweet'], "@[\w]*")

ds.head()

#Removing extra alphabets such as colon, brackets etc..
ds['Tidy_Tweets'] = ds['Tidy_Tweets'].str.replace("[^a-zA-Z#]", " ")

ds.head(10)

#removing short words
ds['Tidy_Tweets'] = ds['Tidy_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))

ds.head(10)

#tokenization 

tokenized_tweet = ds['Tidy_Tweets'].apply(lambda x: x.split())

tokenized_tweet.head()

#stemming

from nltk import PorterStemmer

ps = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])

tokenized_tweet.head()

for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])

ds['Tidy_Tweets'] = tokenized_tweet
ds.head()

"""#Word cloud visualtion"""

from wordcloud import WordCloud,ImageColorGenerator
from PIL import Image
import urllib
import requests

all_words_positive = ' '.join(text for text in ds['Tidy_Tweets'][ds['label']==0])

# combining the image with the dataset
Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))

# We use the ImageColorGenerator library from Wordcloud 
# Here we take the color of the image and impose it over our wordcloud
image_colors = ImageColorGenerator(Mask)

# Now we use the WordCloud function from the wordcloud library 
wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_positive)

plt.figure(figsize=(10,20))

# Here we recolor the words from the dataset to the image's color
# recolor just recolors the default colors to the image's blue color
# interpolation is used to smooth the image generated 
plt.imshow(wc.recolor(color_func=image_colors),interpolation="hamming")

plt.axis('off')
plt.show()

all_words_negative = ' '.join(text for text in ds['Tidy_Tweets'][ds['label']==1])

# combining the image with the dataset
Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))

# We use the ImageColorGenerator library from Wordcloud 
# Here we take the color of the image and impose it over our wordcloud
image_colors = ImageColorGenerator(Mask)

# Now we use the WordCloud function from the wordcloud library 
wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_negative)

# Size of the image generated 
plt.figure(figsize=(10,20))

# Here we recolor the words from the dataset to the image's color
# recolor just recolors the default colors to the image's blue color
# interpolation is used to smooth the image generated 
plt.imshow(wc.recolor(color_func=image_colors),interpolation="gaussian")

plt.axis('off')
plt.show()

"""#Hashtags"""

def Hashtags_Extract(x):
    hashtags=[]
    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r'#(\w+)',i)
        hashtags.append(ht)
    
    return hashtags

ht_positive = Hashtags_Extract(ds['Tidy_Tweets'][ds['label']==0])

ht_positive

ht_positive_unnest = sum(ht_positive,[])

ht_negative = Hashtags_Extract(ds['Tidy_Tweets'][ds['label']==1])

ht_negative

ht_negative_unnest = sum(ht_negative,[])
ht_negative_unnest

"""#ploting bar graphs"""

word_freq_positive = nltk.FreqDist(ht_positive_unnest)

word_freq_positive

df_positive = pd.DataFrame({'Hashtags':list(word_freq_positive.keys()),'Count':list(word_freq_positive.values())})

df_positive.head(10)

df_positive_plot = df_positive.nlargest(20,columns='Count')

sns.barplot(data=df_positive_plot,y='Hashtags',x='Count')
sns.despine()

word_freq_negative = nltk.FreqDist(ht_negative_unnest)

word_freq_negative

df_negative = pd.DataFrame({'Hashtags':list(word_freq_negative.keys()),'Count':list(word_freq_negative.values())})

df_negative.head(10)

df_negative_plot = df_negative.nlargest(20,columns='Count')

sns.barplot(data=df_negative_plot,y='Hashtags',x='Count')
sns.despine()

"""#bag of words """

from sklearn.feature_extraction.text import CountVectorizer

bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')

# bag-of-words feature matrix
bow = bow_vectorizer.fit_transform(ds['Tidy_Tweets'])

df_bow = pd.DataFrame(bow.todense())

df_bow

"""# TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000,stop_words='english')

tfidf_matrix=tfidf.fit_transform(ds['Tidy_Tweets'])

df_tfidf = pd.DataFrame(tfidf_matrix.todense())

df_tfidf

"""#Splitting our dataset into Training and Validation Set"""

train_bow = bow[:31962]

train_bow.todense()

train_tfidf_matrix = tfidf_matrix[:31962]

train_tfidf_matrix.todense()

from sklearn.model_selection import train_test_split

# BOW features
x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train_data['label'],test_size=0.2,random_state=2)

# TF - IDF features
x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train_data['label'],test_size=0.2,random_state=17)

"""#Applying Machine Learning Models

Here we will use 5 different models
Logistic Regression
XGBoost
Decision Trees
Naive Bayes 
KNN
"""

#using f1 score instead of accuracy
from sklearn.metrics import f1_score

"""#Logistic Regression

**BOW**
"""

from sklearn.linear_model import LogisticRegression
Log_Reg = LogisticRegression(random_state=0,solver='lbfgs')

# Fitting LR
Log_Reg.fit(x_train_bow,y_train_bow)

#Prediction
prediction_bow = Log_Reg.predict_proba(x_valid_bow)

prediction_bow

#Calculating the F1 score
#if prediction is greater than or equal to 0.3 than 1 else 0
#Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets

prediction_int = prediction_bow[:,1]>=0.5

#converting the results to integer type
#prediction_bow =prediction_bow(np.int) 

#calculating f1 score
f1_lr_bow= f1_score(y_valid_bow,prediction_int)

print('F1 Score = '+str('{:04.2f}'.format(f1_lr_bow*100) +' %'))
print('\n')

cm= confusion_matrix(y_valid_bow,prediction_int)
fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='rainbow')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="w")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""TFIDF"""

#fitting LR
Log_Reg.fit(x_train_tfidf,y_train_tfidf)

#Prediction
prediction_tfidf = Log_Reg.predict_proba(x_valid_tfidf)

prediction_tfidf

#Calculating F1
# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
prediction_int = prediction_tfidf[:,1]>=0.5
print(prediction_int)
prediction_int = prediction_int.astype(np.int)
print(prediction_int)

# calculating f1 score
f1_lr_tf = f1_score(y_valid_tfidf, prediction_int)

print('F1 Score = '+str('{:04.2f}'.format(f1_lr_tf*100) +' %'))
print('\n')

cm= confusion_matrix(y_valid_tfidf, prediction_int)
fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='rainbow')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="w")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""---



#XG Boost

**BOW**
"""

from xgboost import XGBClassifier

model_bow = XGBClassifier(random_state=22,learning_rate=0.9)

model_bow.fit(x_train_bow, y_train_bow)

xgb = model_bow.predict_proba(x_valid_bow)

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
xgb=xgb[:,1]>=0.5

# converting the results to integer type
xgb_int=xgb.astype(np.int)

# calculating f1 score
f1_xgb_bow=f1_score(y_valid_bow,xgb_int)

print('F1 Score = '+str('{:04.2f}'.format(f1_xgb_bow*100) +' %'))
print("\n")

# Confusion matrix

cm= confusion_matrix(y_valid_bow,xgb_int)
fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='Spectral')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="w")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""**TF-IDF Features**"""

model_tfidf = XGBClassifier(random_state=29,learning_rate=0.7)

model_tfidf.fit(x_train_tfidf, y_train_tfidf)

xgb_tfidf=model_tfidf.predict_proba(x_valid_tfidf)

xgb_tfidf

xgb_tfidf=model_tfidf.predict_proba(x_valid_tfidf)
# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
xgb_tfidf=xgb_tfidf[:,1]>=0.5

# converting the results to integer type
xgb_int_tfidf=xgb_tfidf.astype(np.int)

# calculating f1 score
f1_xgb_tf=f1_score(y_valid_tfidf,xgb_int_tfidf)

print('F1 Score = '+str('{:04.2f}'.format(f1_xgb_tf*100) +' %'))
print('\n')

cm= confusion_matrix(y_valid_tfidf,xgb_int_tfidf)
fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='Spectral')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="w")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""#Decision Trees"""

from sklearn.tree import DecisionTreeClassifier
dct = DecisionTreeClassifier(criterion='entropy', random_state=1)

"""**BOW** """

dct.fit(x_train_bow,y_train_bow)

dct_bow = dct.predict_proba(x_valid_bow)
# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
dct_bow=dct_bow[:,1]>=0.5

# converting the results to integer type
dct_int_bow=dct_bow.astype(np.int)

# calculating f1 score
f1_dct_bow=f1_score(y_valid_bow,dct_int_bow)

print('F1 Score = '+str('{:04.2f}'.format(f1_dct_bow*100))+' %')
print('\n')

cm= confusion_matrix(y_valid_bow,dct_int_bow)


fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='cool')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""**TF-IDF**"""

dct.fit(x_train_tfidf,y_train_tfidf)

dct_tfidf = dct.predict_proba(x_valid_tfidf)

dct_tfidf

dct_tfidf = dct.predict_proba(x_valid_tfidf)
# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
dct_tfidf=dct_tfidf[:,1]>=0.5

# converting the results to integer type
dct_int_tfidf=dct_tfidf.astype(np.int)

# calculating f1 score
f1_dct_tf=f1_score(y_valid_tfidf,dct_int_tfidf)

print('F1 Score = '+str('{:04.2f}'.format(f1_dct_tf*100))+' %')
print('\n')

cm= confusion_matrix(y_valid_tfidf,dct_int_tfidf)


fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='cool')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""# Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

#BOW
MNB  = MultinomialNB()
MNB.fit(x_train_bow,y_train_bow)

from sklearn import metrics
predicted_mnb = MNB.predict_proba(x_valid_bow)
#print(predicted_mnb)

# if prediction is greater than or equal to 0.3 than 1 else 0
# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets
nm_bow_int = predicted_mnb[:,1]>=0.5
#print(prediction_int)
nm_bow_int = nm_bow_int.astype(np.int)
#print(prediction_int)

# calculating f1 score
f1_nb_bow = f1_score(y_valid_bow, nm_bow_int)

print('F1 Score = '+str('{:04.2f}'.format(f1_nb_bow*100))+' %')
print('\n')
cm= confusion_matrix(y_valid_bow,nm_bow_int)


fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='coolwarm')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

# IF-TDF
MNB.fit(x_train_tfidf,y_train_tfidf)

predicted_mnb_tfidf = MNB.predict_proba(x_valid_tfidf)
nm_tf_int = predicted_mnb_tfidf[:,1]>=0.5
#print(prediction_int)
nm_tf_int = nm_tf_int.astype(np.int)
#print(prediction_int)


# calculating f1 score
f1_nb_lr = f1_score(y_valid_tfidf, nm_tf_int)

print('F1 Score = '+str('{:04.2f}'.format(f1_nb_lr*100))+' %')
print('\n')
cm= confusion_matrix(y_valid_tfidf,nm_tf_int)


fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='coolwarm')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

"""# KNN"""

#BOW
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 2)
knn.fit(x_train_bow, y_train_bow)

pred_knn_bow = knn.predict_proba(x_valid_bow)
#print(pred_knn_bow)
knn_bow_int = pred_knn_bow[:,1]>=0.5
knn_bow_int = knn_bow_int.astype(np.int) 
#print(prediction_int)
f1_knn_bow = f1_score(y_valid_bow,knn_bow_int)
print('F1 Score = '+str('{:04.2f}'.format(f1_knn_bow*100))+' %')
print('\n')
cm= confusion_matrix(y_valid_bow,knn_bow_int)


fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='winter_r')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()

# TF IDF
knn.fit(x_train_tfidf, y_train_tfidf)

pred_knn_tfidf = knn.predict_proba(x_valid_tfidf)
knn_tf_int = pred_knn_tfidf[:,1]>=0.5
#print(prediction_int)
knn_tf_int = knn_tf_int.astype(np.int)
#print(prediction_int)


# calculating f1 score
f1_knn_lr = f1_score(y_valid_tfidf, knn_tf_int)
print('F1 Score = '+str('{:04.2f}'.format(f1_knn_lr*100))+' %')
print('\n')
cm= confusion_matrix(y_valid_tfidf,knn_tf_int)


# Confusion matrix
fig, ax = plt.subplots()
im = ax.imshow(cm,cmap ='winter_r')

for i in range(2):
  for j in range(2):
    text = ax.text( j,i,cm[i,j], ha="center", va="center", color="black")

ax.set_title("Actual vs Predicted Confusion matrix")
fig.tight_layout()
plt.colorbar(im)
plt.show()



"""#Model Comparison

**Bag-of-Words**
"""

Algo_1 = ['LogisticRegression(Bag-of-Words)','XGBoost(Bag-of-Words)','DecisionTree(Bag-of-Words)','MultinomialNaiveBayes(Bag-of-Words)','KNN(Bag-of-Words)']

score_1 = [f1_lr_bow,f1_xgb_bow,f1_dct_bow,f1_nb_bow,f1_knn_bow]

compare_1 = pd.DataFrame({'Model':Algo_1,'F1_Score':score_1},index=[i for i in range(1,6)])

compare_1.T

# GRAPH
plt.figure(figsize=(16,9))
sns.set(rc={ 'axes.facecolor':'black','figure.facecolor':'gray'})
sns.pointplot(x='Model',y='F1_Score',data=compare_1,color='r')

plt.title('Bag-of-Words')
plt.xlabel('MODEL')
plt.ylabel('SCORE')

plt.show()

"""**TF-IDF**"""

Algo_2 = ['LogisticRegression(TF-IDF)','XGBoost(TF-IDF)','DecisionTree(TF-IDF)','MultinomialNaiveBayes(TF-IDF)','KNN(TF-IDF)']

score_2 = [f1_lr_tf,f1_xgb_tf,f1_dct_tf,f1_nb_lr,f1_knn_lr]

compare_2 = pd.DataFrame({'Model':Algo_2,'F1_Score':score_2},index=[i for i in range(1,6)])

compare_2.T

#Comparison Graph
plt.figure(figsize=(16,9))
sns.set(rc={ 'axes.facecolor':'black','figure.facecolor':'gray'})
sns.pointplot(x='Model',y='F1_Score',data=compare_2,color='orange')

plt.title('TF-IDF')
plt.xlabel('MODEL')
plt.ylabel('SCORE')

plt.show()

#
Algo_best = ['KNN(Bag-of-Words)','DecisionTree(TF-IDF)']

score_best = [f1_knn_bow,f1_dct_tf]

compare_best = pd.DataFrame({'Model':Algo_best,'F1_Score':score_best},index=[i for i in range(1,3)])

compare_best.T

plt.figure(figsize=(16,9))

sns.pointplot(x='Model',y='F1_Score',data=compare_best,)

plt.title('Best Model(Bag-of-Words & TF-IDF)')
plt.xlabel('MODEL')
plt.ylabel('SCORE')

plt.show()

#Applying the Best model on our data
test_tfidf = tfidf_matrix[31962:]
test_pred = knn.predict_proba(test_tfidf)

test_pred_int = test_pred[:,1] >= 0.5
test_pred_int = test_pred_int.astype(np.int)

test_data['label'] = test_pred_int

submission = test_data[['id','label']]
submission.to_csv('result.csv', index=False)

#Results
res = pd.read_csv('result.csv')
res